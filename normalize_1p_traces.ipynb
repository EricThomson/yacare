{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "def load_template():\n",
    "    \"\"\"\n",
    "    load correlation image saved in data folder\n",
    "    \"\"\"\n",
    "    with open('./data/corr_image.pkl', 'rb') as f:\n",
    "        template = joblib.load(f)['template']\n",
    "    return template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-invention",
   "metadata": {},
   "source": [
    "# Normalizing outputs of CNMFE\n",
    "You have probably noticed that when you run CNMFE on your 1p data, and try to calculate ∆F/F -- `cnm.estimates.F_dff`, which is generated via `cnm.estimates.detrend_df_f()`-- you will get the following warning:\n",
    "\n",
    "    Results should not be interpreted as DF/F normalized but only as detrended.\n",
    "\n",
    "In other words, while the algorithm is *detrending* the traces, it is not actually generating a *normalized* ∆F/F trace for the components. \n",
    "\n",
    "Why is that? It's because the algorithm that caiman uses to normalize the traces with 2p recordings counts on having a well-behaved baseline. This doesn't work for 1p data because the baseline is dominated by out of focus background fluorescence, which makes the problem ill defined (note: I am not 100% sure why it can't be attacked using some iteration of the ring model used in CNMFE, but let's leave that aside for now as caiman doesn't do that).\n",
    "\n",
    "The upshot of this is that caiman doesn't generate a normalized signal for your 1p data, but a signal in arbitrary units that makes it hard to compare magnitudes across components. Luckily, there are other powerful techniques you can use to generate a normalization term for your 1p data that will allow for such comparisons. \n",
    "\n",
    "This notebook shows one such technique that takes advantage of the spectral noise estimation function from caiman `GetSn()`. The technique was suggested originally by Eftychiios Pnevmatikakis, and is adapted from code written by Zach Berry.\n",
    "\n",
    "After setting up the imports and building the cnm object (based on the data used in the caiman demo), we will walk through the concepts involved, and then demo some code that shows how to use this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import welch\n",
    "import joblib\n",
    "import bokeh.plotting as bpl\n",
    "import holoviews as hv\n",
    "\n",
    "import caiman as cm\n",
    "from caiman.source_extraction.cnmf.cnmf import load_CNMF\n",
    "from caiman.source_extraction import cnmf\n",
    "from caiman.source_extraction.cnmf.deconvolution import GetSn\n",
    "\n",
    "bpl.output_notebook()\n",
    "hv.notebook_extension('bokeh')\n",
    "plt.rcParams[\"font.size\"] = \"14\"\n",
    "print(f\"Using python {sys.version}\")\n",
    "print(f\"Holoviews {hv.__version__}\\nCaiman {cm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-scroll",
   "metadata": {},
   "source": [
    "## Build cnmf object\n",
    "Start cluster and build object from `hdf5` file saved from data already analyzed in the CNMFE demo in caiman:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmfe_results_path = r'./data/cnmfe_demo.hdf5'\n",
    "num_cpus = 2 # use however many you need: just leave a few so your RAM stays happy\n",
    "# note if a cluster already exists it will be closed so a new session will be opened\n",
    "if 'dview' in locals():  # locals contains list of current local variables\n",
    "    cm.stop_server(dview=dview)\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(backend='local', \n",
    "                                                 n_processes=num_cpus, \n",
    "                                                 single_thread=False,\n",
    "                                                 ignore_preexisting=True)\n",
    "#Number of nodes in cluster \n",
    "print(f\"Cluster has {n_processes} processes in the pool\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm = load_CNMF(cnmfe_results_path, \n",
    "                n_processes, \n",
    "                dview=dview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-stadium",
   "metadata": {},
   "source": [
    "### Extract basic info from estimates object\n",
    "Get traces, residuals, and reconstruct raw traces from cnmf object. Also, reconstruct array of frame times. \n",
    "\n",
    "Note we will focus on the \"good\" components in `cnm.estimates.idx_components` (for more on interpreting the properties in `cnm.estimates`, see https://caiman.readthedocs.io/en/master/Getting_Started.html#result-interpretation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_inds = cnm.estimates.idx_components\n",
    "\n",
    "denoised_traces = cnm.estimates.C[good_inds,:]  #num_components x num_frames\n",
    "residuals = cnm.estimates.YrA[good_inds,:]\n",
    "raw_traces = denoised_traces + residuals\n",
    "\n",
    "num_components = denoised_traces.shape[0]\n",
    "num_samples = denoised_traces.shape[1]\n",
    "frame_rate = cnm.params.data['fr']\n",
    "sampling_pd = 1/frame_rate\n",
    "num_frames = num_samples\n",
    "frame_times = np.arange(0, sampling_pd*num_samples, sampling_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-rehabilitation",
   "metadata": {},
   "source": [
    "### Plot raw/denoised trace\n",
    "Plot individual traces by hand if you want to keep things simple and use matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ind = 0\n",
    "raw_trace = raw_traces[trace_ind,:]\n",
    "denoised_trace = denoised_traces[trace_ind,:]\n",
    "residual_trace = residuals[trace_ind,:] \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(frame_times, denoised_trace, color='k', linewidth=0.75, label='denoised')\n",
    "plt.plot(frame_times, raw_trace, color='r', linewidth=0.5, label='raw')\n",
    "plt.title(f\"Denoised/Raw Traces (Component {trace_ind})\")\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.legend()\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Activity (au)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20434f40",
   "metadata": {},
   "source": [
    "For more fancy interactive plots, you can also use caimain's built-in viewer to view the spatial footprint and trace for any component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c63ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = load_template()\n",
    "cnm.estimates.hv_view_components(img=template, \n",
    "                                 idx=good_inds,\n",
    "                                 denoised_color='red', \n",
    "                                 cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6640634",
   "metadata": {},
   "source": [
    "## Examine the the power spectral density\n",
    "Intuitively, when you inspect your calcium traces, the *signal* is dominated by lower-frequency, large-amplitude  fluctuations. The *noise* tends to consist of low-amplitude, high-frquency fluctuations. We can look at this in the power spectrum . Let's visualize this for a trace of our choosing (note that the frequency output for `welch()` is given in fraction of sampling frequency, so the maximum is `0.5`, the Nyquist frequency):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2aeb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute power spectral density\n",
    "trace_ind = 0\n",
    "raw_trace = raw_traces[trace_ind,:]\n",
    "ps_freq, ps_dens = welch(raw_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31161c7f",
   "metadata": {},
   "source": [
    "Plot the psd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.semilogy(ps_freq, ps_dens, marker='.', color='k');\n",
    "plt.autoscale(enable=True, axis = 'x', tight = True)\n",
    "plt.axvspan(0.25, 0.5, color='r', alpha=0.1)\n",
    "plt.axvspan(0, 0.25, color='b', alpha=0.1)\n",
    "plt.title(\"Power spectral density\")\n",
    "plt.xlabel(\"Frequency (fraction of sampling frequency)\")\n",
    "plt.ylabel(\"Power/Hz\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d16062",
   "metadata": {},
   "source": [
    "The lower-frequency region of the psd (highlighted in blue) has higher power than the lower frequencies (highlighted in red): note the y-axis is logarithmic so this is actually quite pronounced. This insight was exploited in the original CNMF paper (https://pubmed.ncbi.nlm.nih.gov/26774160/). When discussing how to estimate the noise of a trace (p 296):\n",
    "\n",
    "> [it] can be obtained by observing the power spectral density (PSD) of [the observed calcium signal] y. The uncorrelated additive noise has flat PSD, whereas the PSD due to the calcium signal decays with the frequency as ∼(1/f2). At high frequencies, and under sparse spiking, the PSD will be dominated by the noise, and therefore an estimate σˆ2 can be obtained by averaging the PSD over a range of high frequencies. \n",
    "\n",
    "In practice, what we can do is get the average power in the high (red) frequency range, and take the square root to get an estimate of the standard deviation. This is what the function `GetSn()` does in caiman:\n",
    "\n",
    "https://github.com/flatironinstitute/CaImAn/blob/79bfef180c5b2f9a0d3b65eb0618f1154d692f31/caiman/source_extraction/cnmf/deconvolution.py#L1030\n",
    "\n",
    "If you walk through the code in `GetSn()` you might notice a few wrinkles. In particular:\n",
    "- You can use the `mean`, `median`, or default `logmexp` method to get the \"average\" value in the red frequency range. Because the power spectrum records values across multiple orders of magnitude, the default -- that initially takes a log transform -- tends to work pretty well. \n",
    "- There is an initial division of the power by 2 in `GetSn()` because power in band-limited frequencies includes both positive and negative frequencies so is double the actual value at first (https://en.wikipedia.org/wiki/Spectral_density). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adfbd0d",
   "metadata": {},
   "source": [
    "### The upshot: creating a zscore\n",
    "What we can do with our 1p data is to feed a raw trace into `GetSn()`, retrieve an estimate of our noise, and then use that as our normalization term to get a *normalized* (not just detrended) 1p calcium trace.\n",
    "\n",
    "What is nice about this is you don't have to go through your traces and find regions of \"low signal\" (e.g., people often will try to eyeball their signals and say \"Oh it looks like things are sort of flat here at the beginning of the trace or at this time during the trace: let's pick that as our baseline period\"). Using the power spectral density removes the need to pick artibrary baseline periods from your experiments. \n",
    "\n",
    "The following function `zscore_trace()` calculates uses `GetSn()` to extract the noise based on the power spectral density. The trace is (optionally) shifted by some amount, and then normalized to the noise term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_trace(denoised_trace, \n",
    "                 raw_trace, \n",
    "                 offset_method = 'floor', \n",
    "                 sn_method='logmexp', \n",
    "                 range_ff=[0.25, 0.5]):\n",
    "    \"\"\"\n",
    "    \"Z-score\" calcium traces based on a calculation of noise from\n",
    "    power spectral density high frequency.\n",
    "\n",
    "    Inputs:\n",
    "        denoised_trace: from estimates.C\n",
    "        raw_trace: from estimates.C + estimates.YrA\n",
    "        offset_method: offset to use when shifting the trace (default: 'floor')\n",
    "            'floor': minimum of the trace so new min will be zero\n",
    "            'mean': mean of the raw/denoised trace as the zeroing point\n",
    "            'median': median of raw/denoised trace\n",
    "            'none': no offset just normalize\n",
    "        sn_method: how are psd central values caluclated \n",
    "            mean\n",
    "            median' or 'logmexp')\n",
    "        range_ff: 2-elt array-like range of frequencies (input for GetSn) (default [0.25, 0.5])\n",
    "\n",
    "    Returns \n",
    "        z_denoised: same shape as denoised_trace\n",
    "        z_raw: same shape as raw trace\n",
    "        trace_noise: noise level from z_raw\n",
    "        \n",
    "    Adapted from code by Zach Barry.\n",
    "    \"\"\"\n",
    "    noise = GetSn(raw_trace, range_ff=range_ff, method=sn_method)  #import this from caiman\n",
    "\n",
    "    if offset_method == 'floor':\n",
    "        raw_offset = np.min(raw_trace)\n",
    "        denoised_offset = np.min(denoised_trace)\n",
    "    elif offset_method == 'mean':\n",
    "        raw_offset = np.mean(raw_trace)\n",
    "        denoised_offset = np.mean(denoised_trace)\n",
    "    elif offset_method == 'median':\n",
    "        raw_offset = np.median(raw_trace)\n",
    "        denoised_offset = np.median(denoised_trace)\n",
    "    elif offset_method == 'none':\n",
    "        raw_offset = 0\n",
    "        denoised_offset = 0\n",
    "    else:\n",
    "        raise ValueError(\"offset_method should be floor, mean, median, or none.\")\n",
    "           \n",
    "    z_raw = (raw_trace - raw_offset) / noise\n",
    "    z_denoised = (denoised_trace - denoised_offset)/ noise\n",
    "        \n",
    "    return z_denoised, z_raw, noise\n",
    "\n",
    "print('loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a20278",
   "metadata": {},
   "source": [
    "Applying the function to our trace above, with two different offset methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbcac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_denoised_floor, z_raw_floor, _ = zscore_trace(denoised_trace, \n",
    "                                                raw_trace, \n",
    "                                                offset_method='none', \n",
    "                                                sn_method='logmexp')\n",
    "z_denoised_median, z_raw_median, _ = zscore_trace(denoised_trace, \n",
    "                                                  raw_trace, \n",
    "                                                  offset_method='median', \n",
    "                                                  sn_method='logmexp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69110a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2,1, figsize=(12,6))\n",
    "axes[0].plot(frame_times, z_raw_floor)\n",
    "axes[0].set_title('Floor', fontsize=14)\n",
    "axes[0].grid()\n",
    "axes[1].plot(frame_times, z_raw_median)\n",
    "axes[1].set_title('Median', fontsize=14)\n",
    "axes[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12aa82",
   "metadata": {},
   "source": [
    "Depending on the trace, you will sometimes find significant differences between `floor` and the other two offset methods, so you should figure out what you are most happy with. The main point, though, is that we now have a calcium trace that is normalized to something meaningful: *the intrinsic noise levels in that trace*. We have something likie a traditional z score. This makes it more meaningful to compare magnitudes between components. \n",
    "\n",
    "Once you have settings you like you can then apply it to all traces in your data. E.g., something like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5696a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_traces(cnm_c, \n",
    "                  cnm_yra, \n",
    "                  offset_method = 'floor', \n",
    "                  sn_method = 'logmexp', \n",
    "                  range_ff=[0.25, 0.5]):\n",
    "    \"\"\"\n",
    "    apply zscore_trace to all traces in estimates\n",
    "    \n",
    "    inputs:\n",
    "        cnm_c: C array of denoised traces from cnm.estimates\n",
    "        cnm_yra: YrA array of residuals from cnm.estimate\n",
    "        offset_method: floor/mean/median (see zscore_trace)\n",
    "        sn_method: mean/median/logmexp (see zscore_trace)\n",
    "        range_ff: frequency range for GetSn\n",
    "    \n",
    "    outputs:\n",
    "        denoised_z_traces\n",
    "        raw_z_traces\n",
    "        noise_all\n",
    "    \"\"\"\n",
    "    raw_traces = cnm_c + cnm_yra  # raw_trace[i] = c[i] + yra[i]\n",
    "    raw_z_traces = []\n",
    "    denoised_z_traces = []\n",
    "    noise_all = []\n",
    "    for ind, raw_trace in enumerate(raw_traces):\n",
    "        denoised_trace = cnm_c[ind,:]\n",
    "        z_denoised, z_raw, noise = zscore_trace(denoised_trace,\n",
    "                                                raw_trace, \n",
    "                                                offset_method=offset_method, \n",
    "                                                sn_method = sn_method,\n",
    "                                                range_ff=range_ff)\n",
    "        \n",
    "        denoised_z_traces.append(z_denoised)\n",
    "        raw_z_traces.append(z_raw)\n",
    "        noise_all.append(noise)\n",
    "        \n",
    "    denoised_z_traces = np.array(denoised_z_traces)\n",
    "    raw_z_traces = np.array(raw_z_traces)\n",
    "    noise_all = np.array(noise_all)\n",
    "    \n",
    "    return denoised_z_traces, raw_z_traces, noise_all\n",
    "\n",
    "print('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e38fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_z_traces, raw_z_traces, noise_all = zscore_traces(denoised_traces,\n",
    "                                                           residuals, \n",
    "                                                           offset_method='floor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7822c",
   "metadata": {},
   "source": [
    "Pick one and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ae9b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ind = 1\n",
    "denoised_z_trace = denoised_z_traces[trace_ind,:]\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(frame_times, denoised_z_trace, color='k', linewidth=0.75)\n",
    "# plt.axhspan(-3, 3, color='lime', alpha=0.2);\n",
    "plt.title(f'Normalized Activity in Component {trace_ind}', fontsize=16)\n",
    "plt.xlabel('Time (s)', fontsize=14)\n",
    "plt.ylabel('Activity (Normalized)', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d226c45",
   "metadata": {},
   "source": [
    "And once you have your data in this form, you can then use it for all subsequent analysis.\n",
    "\n",
    "`GetSn()`is just one of many gems hidden in the back end of caiman that is extremely useful outside of its original context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c63b6",
   "metadata": {},
   "source": [
    "## Comparison to standard deviation\n",
    "Another common method of normalizing is to just calculate the standard deviation over the entire raw trace. You can then use that as your normalizing term. This will work, though it will tend to overestimate the noise level, because it includes the large-amplitude signal fluctuations in the calculation in addition to the noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev_all = np.std(raw_traces, axis=1, ddof=1);\n",
    "\n",
    "plt.scatter(noise_all, std_dev_all, 8, color='k', alpha=0.4);\n",
    "plt.plot([0, np.max(std_dev_all)], \n",
    "         [0, np.max(std_dev_all)], \n",
    "         linestyle='--', \n",
    "         color='k')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel('Spectral Noise')\n",
    "plt.ylabel('Standard Deviation');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c7af8",
   "metadata": {},
   "source": [
    "It is clear that the spectral method, partly because it limits the signal's bandwidth, ends up putting consistently lower estimates on the noise. When I have played with these methods, for instance by pulling out \"pure noise\" stretches of 1p signals, and calculated the standard deviation on this stretch, the two methods tend to match up fairly closely (though the standard deviation is still typically higher, because the spectral technique is still bandpass-limited). I prefer the spectral technique because by filtering out the stochastic large-amplitude low-frequency fluctuations in the signals: it provides a more consistent estimate of the noise, and doesn't require you to do things like pick a certain region of the signal to count as noise vs signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-forty",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Stop the server to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.stop_server(dview=dview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf478df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
